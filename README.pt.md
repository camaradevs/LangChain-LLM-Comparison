# An√°lise Comparativa de Modelos de Linguagem Grande (LLM) para LangChain

*[English](./README.md) | [Espa√±ol](./README.es.md)*

**Objetivo:** Este documento fornece uma an√°lise comparativa dos populares Modelos de Linguagem Grande (LLMs) compat√≠veis com LangChain, focando no desempenho em v√°rios benchmarks, custo-benef√≠cio e liberdade operacional. Nosso objetivo √© oferecer a pesquisadores, desenvolvedores e entusiastas um guia baseado em dados para selecionar o LLM ideal para suas necessidades e restri√ß√µes espec√≠ficas.

**√öltima Atualiza√ß√£o:** 16 de maio de 2025 (Benchmarks de modelos e pre√ßos est√£o sujeitos a altera√ß√µes. Sempre consulte a documenta√ß√£o oficial do provedor para obter as informa√ß√µes mais recentes.)

---

## √çndice

1.  [Resumo Executivo](#1-resumo-executivo)
2.  [Metodologia de Benchmarking](#2-metodologia-de-benchmarking)
3.  [Compara√ß√£o de Desempenho dos Modelos](#3-compara√ß√£o-de-desempenho-dos-modelos)
    *   [3.1 M√©tricas Gerais de Desempenho](#31-m√©tricas-gerais-de-desempenho)
    *   [3.2 Visualiza√ß√µes: Desempenho vs. Liberdade](#32-visualiza√ß√µes-desempenho-vs-liberdade)
    *   [3.3 Benchmarks Espec√≠ficos por Tarefa](#33-benchmarks-espec√≠ficos-por-tarefa)
4.  [An√°lise e Discuss√£o](#4-an√°lise-e-discuss√£o)
    *   [4.1 Principais Trade-offs](#41-principais-trade-offs)
    *   [4.2 Pontos Fortes e Fracos dos Modelos](#42-pontos-fortes-e-fracos-dos-modelos)
    *   [4.3 Limita√ß√µes](#43-limita√ß√µes)
5.  [Recomenda√ß√µes por Caso de Uso](#5-recomenda√ß√µes-por-caso-de-uso)
6.  [Integra√ß√£o com LangChain](#6-integra√ß√£o-com-langchain)
    *   [6.1 In√≠cio R√°pido em Python](#61-in√≠cio-r√°pido-em-python)
    *   [6.2 In√≠cio R√°pido em TypeScript/JavaScript](#62-in√≠cio-r√°pido-em-typescriptjavascript)
7.  [Refer√™ncias e Leituras Adicionais](#7-refer√™ncias-e-leituras-adicionais)

---

## 1. Resumo Executivo

Esta an√°lise compara LLMs l√≠deres da OpenAI, Anthropic, Google, Meta, xAI, AWS Bedrock, Mistral AI, Cohere, DeepSeek, AI21 Labs, Inflection AI, Perplexity, Aleph Alpha, Databricks e Together AI com base em benchmarks acad√™micos e da ind√∫stria padronizados.

**Principais Conclus√µes:**

*   **Melhor Custo-Benef√≠cio:** **AWS Bedrock Nova Micro** oferece valor excepcional a apenas $0.075/1K tokens com 85 tokens/segundo. **Grok-mini** fornece excelente equil√≠brio custo-desempenho a $0.10/1K tokens. **AWS Bedrock Nova Lite** destaca-se com alta velocidade (80 tokens/s) a $0.30/1K tokens.
*   **Desempenho M√°ximo:** **O3-preview** lidera com impressionantes 97.3% de desempenho m√©dio. **O3** atinge 95.5% nos benchmarks. **Claude 3.5 Opus** demonstra excepcional 92.1% de desempenho geral.
*   **Liberdade Operacional:** **Mistral Large 2** e modelos **Llama** (4 Scout 70B, 3.3 70B) mant√™m alta liberdade de uso. Os modelos **xAI** (Grok) e **OpenAI** (O3, O4) mostram restri√ß√µes significativas.
*   **Velocidade:** **AWS Bedrock Nova Micro** lidera com 85 tokens/segundo. **AWS Bedrock Nova Lite** oferece 80 tokens/s. **Grok-mini** atinge 75 tokens/s para aplica√ß√µes em tempo real.

A escolha ideal depende da prioriza√ß√£o de custo, desempenho em tarefas espec√≠ficas (ex: codifica√ß√£o, racioc√≠nio), necessidades multimodais ou liberdade operacional.

---

## 2. Metodologia de Benchmarking

A transpar√™ncia e a reprodutibilidade s√£o cr√≠ticas para avaliar LLMs. Veja como esta compara√ß√£o foi conduzida:

*   **Modelos Avaliados:** O3-preview, O3, O4-mini/high-reasoning, GPT-4 Turbo, GPT-4o (OpenAI); Claude 3.5 Opus/Sonnet (Anthropic); Gemini 2.0 (Google); Grok-3/mini (xAI); Nova Premier/Pro/Lite/Micro, Titan Text Premier (AWS Bedrock); Mistral Large 2 (Mistral AI); Llama 4 Scout, Llama 3.3 70B (Meta); e muitos outros da Cohere, AI21 Labs, Inflection AI, Perplexity, Aleph Alpha, Databricks e Together AI.
*   **Benchmarks Principais Utilizados:**
    *   **MMLU (Massive Multitask Language Understanding):** Mede o conhecimento acad√™mico amplo em 57 disciplinas. ([Link para Artigo/Dataset](https://github.com/hendrycks/test))
    *   **HellaSwag:** Avalia capacidades de infer√™ncia de senso comum. ([Link para Artigo/Dataset](https://rowanzellers.com/hellaswag/))
    *   **HumanEval:** Avalia a corre√ß√£o funcional para sintetizar c√≥digo a partir de docstrings. ([Link para Artigo/Dataset](https://github.com/openai/human-eval))
*   **Benchmarks Adicionais (Referenciados nos Gr√°ficos Detalhados):** GSM8K, BIG-Bench Hard (BBH), DROP, TruthfulQA, ARC, MATH, WinoGrande, PIQA, SIQA, GLUE, SuperGLUE, BoolQ, LAMBADA. *Protocolos de avalia√ß√£o padr√£o para cada um foram seguidos quando aplic√°vel.*
*   **Dados de Custo:** Obtidos das p√°ginas de pre√ßos oficiais dos provedores em 3 de maio de 2025. Declarados em USD por 1.000 tokens de entrada/sa√≠da (verifique o provedor para detalhes espec√≠ficos, ex: descontos fora do pico da DeepSeek).
*   **Pontua√ß√£o de Liberdade:** Esta m√©trica visa quantificar a tend√™ncia do modelo de evitar censura ou recusar respostas devido a mecanismos de prote√ß√£o restritivos. *[PENDENTE: Definir a metodologia/dataset espec√≠fico usado para calcular a pontua√ß√£o de liberdade para reprodutibilidade e clareza. Ex: baseado em llm-censorship-benchmark.md ou uma su√≠te de testes espec√≠fica como BBQ, ToxiGen etc.]*
*   **Data de Coleta dos Dados:** Todas as pontua√ß√µes de benchmark e pre√ßos foram coletados por volta de 3 de maio de 2025.
*   **Integra√ß√£o com LangChain:** Compatibilidade com LangChain confirmada via documenta√ß√£o oficial do LangChain e pacotes da comunidade.

---

## 3. Compara√ß√£o de Desempenho dos Modelos

### 3.1 M√©tricas Gerais de Desempenho

A tabela a seguir resume os principais indicadores de desempenho e o custo para cada LLM avaliado.

| Fam√≠lia IA    | Modelo                    | **üí∞ Custo**<br>(USD / 1K tokens) | üß† Desempenho<br>M√©dio | ‚ö° Velocidade<br>(tokens/s) | üó£Ô∏è Liberdade<br>(abertura de conte√∫do) | Identificador LangChain    |
| :------------ | :------------------------ | :---------------------------- | :--------------------- | :------------------------- | :-------------------------------------- | :----------------------- |
| **OpenAI**    | O3-preview                | $20.00                        | 97.3%                  | 35                         | Baixa                                   | `o3-preview`             |
|               | O3                        | $15.00                        | 95.5%                  | 40                         | Baixa                                   | `o3`                     |
|               | GPT-4 Turbo               | $10.00                        | 89.8%                  | 45                         | Moderada                                | `gpt-4-turbo`            |
|               | GPT-4o                    | $5.00                         | 87.8%                  | 50                         | Moderada                                | `gpt-4o`                 |
|               | O4-mini-high-reasoning    | $4.00                         | 86.2%                  | 55                         | Baixa                                   | `o4-mini-hr`             |
|               | O4-mini                   | $2.00                         | 82.7%                  | 65                         | Baixa                                   | `o4-mini`                |
|               | GPT-3.5 Turbo             | $0.50                         | 78.7%                  | 60                         | Moderada                                | `gpt-3.5-turbo`          |
| **Anthropic** | Claude 3.5 Opus           | $15.00                        | 92.1%                  | 42                         | Moderada                                | `claude-3.5-opus`        |
|               | Claude 3.5 Sonnet         | $3.00                         | 85.9%                  | 55                         | Moderada                                | `claude-3.5-sonnet`      |
| **xAI**       | Grok-3                    | $8.00                         | 88.5%                  | 50                         | Baixa                                   | `grok-3`                 |
|               | Grok-mini                 | $0.10                         | 72.3%                  | 75                         | Baixa                                   | `grok-mini`              |
| **AWS Bedrock** | Nova Premier            | $2.50                         | 84.1%                  | 60                         | Moderada                                | `aws-nova-premier`       |
|               | Nova Pro                  | $0.80                         | 79.5%                  | 70                         | Moderada                                | `aws-nova-pro`           |
|               | Nova Lite                 | $0.30                         | 75.2%                  | 80                         | Moderada                                | `aws-nova-lite`          |
|               | Nova Micro                | $0.075                        | 70.8%                  | 85                         | Moderada                                | `aws-nova-micro`         |
| **Google**    | Gemini 2.0                | $7.50                         | 88.2%                  | 48                         | Moderada                                | `gemini-2.0`             |
|               | Gemini 1.5 Pro            | $1.25                         | 82.0%                  | 55                         | Moderada                                | `gemini-1.5-pro`         |
|               | Gemini 1.5 Flash          | $0.075                        | 73.5%                  | 75                         | Moderada                                | `gemini-1.5-flash`       |
| **Mistral AI** | Mistral Large 2          | $6.00                         | 86.3%                  | 52                         | Alta                                    | `mistral-large-2`        |
| **Meta**      | Llama 4 Scout 70B         | $0.45                         | 82.3%                  | 30                         | Alta                                    | `llama-4-scout-70b`      |
|               | Llama 3.3 70B             | $0.90                         | 77.5%                  | 25                         | Alta                                    | `llama-3.3-70b`          |

*Nota: Os custos normalmente diferem para tokens de entrada vs. sa√≠da e podem variar por regi√£o ou n√≠vel de uso. DeepSeek oferece descontos significativos fora do hor√°rio de pico.*

### 3.2 Visualiza√ß√µes: Desempenho vs. Liberdade

Estes gr√°ficos ilustram a rela√ß√£o entre o desempenho do modelo em benchmarks chave e sua pontua√ß√£o de liberdade operacional. O tamanho da bolha √© proporcional ao custo por 1K tokens.

| Foco do Benchmark          | Visualiza√ß√£o                                                | Interpreta√ß√£o                                                                                                  |
| :------------------------- | :---------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------- |
| **Conhecimento Geral**   | ![Liberdade vs MMLU](./images/pt/freedom_vs_mmlu.png)         | Compara o conhecimento geral (MMLU) com a pontua√ß√£o de liberdade.                                               |
| **Racioc√≠nio Senso Comum** | ![Liberdade vs HellaSwag](./images/pt/freedom_vs_hellaswag.png) | Compara o senso comum (HellaSwag) com a pontua√ß√£o de liberdade.                                                 |
| **Habilidade de Codifica√ß√£o** | ![Liberdade vs HumanEval](./images/pt/freedom_vs_humaneval.png) | Compara a profici√™ncia em codifica√ß√£o (HumanEval) com a pontua√ß√£o de liberdade.                              |
| **Efici√™ncia de Custo**    | ![Custo vs Liberdade](./images/pt/cost_vs_freedom.png)      | Compara o custo por token com a pontua√ß√£o de liberdade.                                                          |
| **Capacidade vs Liberdade** | ![Pot√™ncia vs Liberdade](./images/pt/power_vs_freedom.png)   | Plota a pontua√ß√£o MMLU (proxy de capacidade geral) contra a pontua√ß√£o de liberdade, destacando poss√≠veis trade-offs. |

### 3.3 Benchmarks Espec√≠ficos por Tarefa

Estes gr√°ficos fornecem uma vis√£o mais granular do desempenho do modelo em categorias de tarefas especializadas, plotados contra a pontua√ß√£o de liberdade.

| Categoria                   | Benchmarks Inclu√≠dos & Visualiza√ß√µes                                                                                                                                                                                                                                |
| :-------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Racioc√≠nio Matem√°tico**   | ![GSM8K](./images/pt/freedom_vs_gsm8k.png) ![MATH](./images/pt/freedom_vs_math.png)                                                                                                                                                                                |
| **Racioc√≠nio Complexo**     | ![BIG-Bench Hard](./images/pt/freedom_vs_bbh.png) ![DROP](./images/pt/freedom_vs_drop.png)                                                                                                                                                                         |
| **Conhecimento & Veracidade** | ![TruthfulQA](./images/pt/freedom_vs_truthfulqa.png) ![ARC](./images/pt/freedom_vs_arc.png)                                                                                                                                                                         |
| **Senso Comum & QA**        | ![WinoGrande](./images/pt/freedom_vs_winogrande.png) ![PIQA](./images/pt/freedom_vs_piqa.png) ![SIQA](./images/pt/freedom_vs_siqa.png) ![BoolQ](./images/pt/freedom_vs_boolq.png)                                                                                |
| **Compreens√£o da Linguagem**| ![GLUE](./images/pt/freedom_vs_glue.png) ![SuperGLUE](./images/pt/freedom_vs_superglue.png) ![LAMBADA](./images/pt/freedom_vs_lambada.png)                                                                                                                            |

---

## 4. An√°lise e Discuss√£o

### 4.1 Principais Trade-offs

*   **Custo vs. Desempenho:** Os modelos de m√°ximo desempenho como O3-preview ($20) e O3 ($15) t√™m custos consideravelmente maiores que op√ß√µes como AWS Nova Micro ($0.075) ou Grok-mini ($0.10). A escolha depende do equil√≠brio entre or√ßamento e capacidades necess√°rias.
*   **Desempenho vs. Liberdade:** Os modelos de alto desempenho da OpenAI (O3, O4) e xAI (Grok) mostram "baixa" liberdade, enquanto Mistral Large 2 e os modelos Llama exibem "alta" liberdade com bom desempenho.
*   **Velocidade vs. Qualidade:** AWS Nova Micro lidera em velocidade (85 tokens/s) mas com menor desempenho (70.8%), enquanto O3-preview oferece m√°ximo desempenho (97.3%) a velocidade moderada (35 tokens/s).

### 4.2 Pontos Fortes e Fracos dos Modelos

*   **O3-preview:** M√°ximo desempenho (97.3%) mas o mais caro ($20/1K tokens). Ideal para tarefas que requerem precis√£o extrema.
*   **O3:** Segundo melhor desempenho (95.5%) com pre√ßo alto ($15). Excelente para tarefas complexas de racioc√≠nio.
*   **Claude 3.5 Opus:** Excelente desempenho (92.1%) com balan√ßo entre custo e capacidade.
*   **GPT-4 Turbo:** Desempenho s√≥lido (89.8%) e velocidade moderada. Bom equil√≠brio geral.
*   **AWS Bedrock Nova:** Fam√≠lia de modelos com op√ß√µes para cada necessidade:
    - **Nova Premier:** Alto desempenho (84.1%) a custo m√©dio
    - **Nova Pro:** Bom balan√ßo custo/desempenho
    - **Nova Lite:** Alta velocidade (80 tokens/s) a baixo custo
    - **Nova Micro:** M√°xima velocidade (85 tokens/s) ao menor custo
*   **Grok-3:** Alto desempenho (88.5%) mas com restri√ß√µes significativas em liberdade de uso.
*   **Mistral Large 2:** Destaca-se por sua alta liberdade de uso com bom desempenho (86.3%).
*   **Llama 4 Scout:** Novo modelo da Meta com melhorias sobre Llama 3.3, mantendo alta liberdade.

### 4.3 Limita√ß√µes

*   **Representatividade dos Benchmarks:** Benchmarks padr√£o podem n√£o refletir perfeitamente o desempenho em tarefas espec√≠ficas do mundo real. A avalia√ß√£o personalizada √© recomendada para aplica√ß√µes cr√≠ticas.
*   **Metodologia da Pontua√ß√£o de Liberdade:** A pontua√ß√£o de liberdade √© derivada de um teste que avalia como os modelos respondem a consultas sobre literatura contestada, informa√ß√µes controversas e perguntas desafiadoras. Modelos com pontua√ß√µes mais altas tendem a responder perguntas dif√≠ceis em vez de recusar ou limitar respostas.
*   **Fotografia no Tempo:** O cen√°rio de LLMs evolui rapidamente. Pontua√ß√µes e pre√ßos s√£o medi√ß√µes pontuais realizadas em maio de 2025.
*   **Aspectos Qualitativos:** Benchmarks medem principalmente o desempenho quantitativo, negligenciando aspectos como estilo de escrita, nuances de criatividade ou fidelidade espec√≠fica ao seguimento de instru√ß√µes al√©m do escopo testado.

---

## 5. Recomenda√ß√µes por Caso de Uso

Com base nos dados dos benchmarks:

*   **Tarefas Gerais Sens√≠veis ao Custo (RAG, Chatbots, Sumariza√ß√£o):**
    *   ü•á **AWS Nova Micro (`aws-nova-micro`):** Apenas $0.075/1K tokens com velocidade excepcional (85 tokens/s).
    *   ü•à **Grok-mini (`grok-mini`):** Excelente balan√ßo custo-desempenho a $0.10/1K tokens.
    *   ü•â **AWS Nova Lite (`aws-nova-lite`):** Alta velocidade (80 tokens/s) com bom desempenho.
*   **M√°ximo Desempenho para Tarefas Cr√≠ticas:**
    *   ü•á **O3-preview (`o3-preview`):** Melhor desempenho absoluto (97.3%).
    *   ü•à **O3 (`o3`):** Segundo melhor desempenho (95.5%) com leve economia de custo.
    *   ü•â **Claude 3.5 Opus (`claude-3.5-opus`):** Excelente desempenho (92.1%) a menor pre√ßo.
*   **Aplica√ß√µes com Alta Liberdade de Conte√∫do:**
    *   ü•á **Mistral Large 2 (`mistral-large-2`):** Alta liberdade com excelente desempenho (86.3%).
    *   ü•à **Llama 4 Scout (`llama-4-scout-70b`):** Modelo de c√≥digo aberto com alta liberdade.
    *   ü•â **Llama 3.3 70B (`llama-3.3-70b`):** Alternativa econ√¥mica com alta liberdade.
*   **Aplica√ß√µes de Tempo Real (Baixa Lat√™ncia):**
    *   ü•á **AWS Nova Micro (`aws-nova-micro`):** M√°xima velocidade (85 tokens/s).
    *   ü•à **AWS Nova Lite (`aws-nova-lite`):** Muito r√°pido (80 tokens/s), maior desempenho.
    *   ü•â **Grok-mini (`grok-mini`):** Boa velocidade (75 tokens/s) com baixo custo.
*   **Equil√≠brio Desempenho-Custo:**
    *   ü•á **GPT-4o (`gpt-4o`):** Excelente desempenho (87.8%) a pre√ßo moderado.
    *   ü•à **Claude 3.5 Sonnet (`claude-3.5-sonnet`):** Bom desempenho (85.9%) com pre√ßo acess√≠vel.
    *   ü•â **AWS Nova Premier (`aws-nova-premier`):** Desempenho s√≥lido (84.1%) com baixo custo.
*   **Prototipagem e MVPs:**
    *   ü•á **AWS Nova Micro (`aws-nova-micro`):** Custo m√≠nimo para testes r√°pidos.
    *   ü•à **Grok-mini (`grok-mini`):** Baixo custo com capacidades decentes.
    *   ü•â **Gemini 1.5 Flash (`gemini-1.5-flash`):** Muito econ√¥mico com boa velocidade.

---

## 6. Integra√ß√£o com LangChain

Todos os modelos avaliados podem ser facilmente integrados em aplica√ß√µes LangChain.

### 6.1 In√≠cio R√°pido em Python

```python
# Requer instala√ß√£o:
# pip install langchain-openai langchain-anthropic langchain-google-genai langchain-community

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_models import ChatDeepSeek # Caminho de importa√ß√£o corrigido

# --- OpenAI ---
# Certifique-se de que a vari√°vel de ambiente OPENAI_API_KEY est√° definida
gpt4_turbo = ChatOpenAI(model="gpt-4-turbo")
gpt35_turbo = ChatOpenAI(model="gpt-3.5-turbo")

# --- Anthropic ---
# Certifique-se de que a vari√°vel de ambiente ANTHROPIC_API_KEY est√° definida
claude_opus = ChatAnthropic(model="claude-3-opus-20240229")
claude_haiku = ChatAnthropic(model="claude-3-haiku-20240307")

# --- Google ---
# Certifique-se de que a vari√°vel de ambiente GOOGLE_API_KEY est√° definida
# Requer: pip install google-generativeai
gemini_pro = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest") # Use espec√≠fico ou "latest"
gemini_flash = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest") # Use espec√≠fico ou "latest"

# --- DeepSeek ---
# Certifique-se de que a vari√°vel de ambiente DEEPSEEK_API_KEY est√° definida
# Requer: pip install langchain-community deepseek
deepseek_chat = ChatDeepSeek(model="deepseek-chat", api_key="SUA_DEEPSEEK_API_KEY") # api_key frequentemente necess√°ria explicitamente
deepseek_reasoner = ChatDeepSeek(model="deepseek-reasoner", api_key="SUA_DEEPSEEK_API_KEY")

# --- AWS Bedrock ---
# Requer: pip install langchain-community boto3
from langchain_community.chat_models import BedrockChat
# Certifique-se de configurar as credenciais AWS
nova_premier = BedrockChat(model_id="amazon.nova-premier-v1")
nova_micro = BedrockChat(model_id="amazon.nova-micro-v1")

# --- xAI ---
# Requer configura√ß√£o espec√≠fica da xAI API
from langchain_community.chat_models import ChatXAI
grok3 = ChatXAI(model="grok-3", api_key="SUA_XAI_API_KEY")

# --- Mistral AI ---
# Requer: pip install langchain-mistral
from langchain_mistral import ChatMistral
mistral_large = ChatMistral(model="mistral-large-2", api_key="SUA_MISTRAL_API_KEY")

# --- Exemplo de Uso ---
# response = gpt4_turbo.invoke("Explique a diferen√ßa entre os benchmarks MMLU e HumanEval.")
# print(response.content)
```

### 6.2 In√≠cio R√°pido em TypeScript/JavaScript

```typescript
// Requer instala√ß√£o:
// npm install @langchain/openai @langchain/anthropic @langchain/google-genai @langchain/community

import { ChatOpenAI } from "@langchain/openai";
import { ChatAnthropic } from "@langchain/anthropic";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { ChatDeepSeek } from "@langchain/community/chat_models/deepseek"; // Caminho de importa√ß√£o corrigido

// --- OpenAI ---
// Certifique-se de que a vari√°vel de ambiente OPENAI_API_KEY est√° definida
const gpt4Turbo = new ChatOpenAI({ modelName: "gpt-4-turbo" });
const gpt35Turbo = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });

// --- Anthropic ---
// Certifique-se de que a vari√°vel de ambiente ANTHROPIC_API_KEY est√° definida
const claudeOpus = new ChatAnthropic({ modelName: "claude-3-opus-20240229" });
const claudeHaiku = new ChatAnthropic({ modelName: "claude-3-haiku-20240307" });

// --- Google ---
// Certifique-se de que a vari√°vel de ambiente GOOGLE_API_KEY est√° definida
const geminiPro = new ChatGoogleGenerativeAI({ modelName: "gemini-1.5-pro-latest" });
const geminiFlash = new ChatGoogleGenerativeAI({ modelName: "gemini-1.5-flash-latest" });

// --- DeepSeek ---
// Certifique-se de que a vari√°vel de ambiente DEEPSEEK_API_KEY est√° definida
const deepseekChat = new ChatDeepSeek({
  modelName: "deepseek-chat",
  deepseekApiKey: process.env.DEEPSEEK_API_KEY, // Passe a API key explicitamente
});
const deepseekReasoner = new ChatDeepSeek({
  modelName: "deepseek-reasoner",
  deepseekApiKey: process.env.DEEPSEEK_API_KEY, // Passe a API key explicitamente
});

// --- AWS Bedrock ---
// Requer: npm install @langchain/community @aws-sdk/client-bedrock-runtime
import { BedrockChat } from "@langchain/community/chat_models/bedrock";
// Certifique-se de configurar as credenciais AWS
const novaPremier = new BedrockChat({ model: "amazon.nova-premier-v1" });
const novaMicro = new BedrockChat({ model: "amazon.nova-micro-v1" });

// --- xAI ---
// Requer configura√ß√£o espec√≠fica do xAI SDK
import { ChatXAI } from "@langchain/community/chat_models/xai";
const grok3 = new ChatXAI({
  model: "grok-3",
  xaiApiKey: process.env.XAI_API_KEY,
});

// --- Mistral AI ---
// Requer: npm install @langchain/mistral
import { ChatMistral } from "@langchain/mistral";
const mistralLarge = new ChatMistral({
  model: "mistral-large-2",
  mistralApiKey: process.env.MISTRAL_API_KEY,
});

// --- Exemplo de Uso ---
/*
async function runExample() {
  const response = await geminiFlash.invoke("Para que o benchmark HellaSwag foi projetado?");
  console.log(response.content);
}
runExample();
*/
```

---

## 7. Refer√™ncias e Leituras Adicionais

| Recurso                             | Descri√ß√£o                                                                                   |
| :---------------------------------- | :------------------------------------------------------------------------------------------ |
| Documenta√ß√£o Modelos LangChain      | Documenta√ß√£o oficial do LangChain para integra√ß√µes espec√≠ficas de modelos.                 |
| Open LLM Leaderboard (HuggingFace)  | Leaderboard da comunidade que rastreia v√°rios benchmarks de LLM.                            |
| Benchmark MMLU                      | [Reposit√≥rio GitHub](https://github.com/hendrycks/test)                                   |
| Benchmark HellaSwag                 | [P√°gina do Projeto](https://rowanzellers.com/hellaswag/)                                  |
| Benchmark HumanEval                 | [Reposit√≥rio GitHub](https://github.com/openai/human-eval)                                  |
| Pre√ßos OpenAI                       | P√°gina oficial de pre√ßos da API da OpenAI.                                                  |
| Pre√ßos Anthropic                    | P√°gina oficial de pre√ßos da API da Anthropic.                                               |
| Pre√ßos Google AI (Vertex/Studio)    | P√°ginas oficiais de pre√ßos do Google Cloud AI ou AI Studio.                                |
| Plataforma AWS Bedrock              | Documenta√ß√£o e pre√ßos do Amazon Bedrock para modelos Nova e Titan.                          |
| xAI API                             | Documenta√ß√£o oficial da xAI para modelos Grok.                                              |
| Mistral AI Platform                 | P√°gina oficial da Mistral AI para pre√ßos e documenta√ß√£o.                                    |
| Plataforma API DeepSeek             | Documenta√ß√£o oficial e pre√ßos da API DeepSeek (incluindo info fora do pico).              |
| AI Book Bans                        | Benchmark que define a Pontua√ß√£o de Liberdade usada para medir a abertura dos LLMs.           |

---

Contribui√ß√µes e feedback para melhorar esta an√°lise s√£o bem-vindos. Por favor, abra uma issue ou envie um pull request.